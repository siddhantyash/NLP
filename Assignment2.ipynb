{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,WhitespaceTokenizer,TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwords = set(stopwords.words('english'))\n",
    "panctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ('Then, beginning late last month, for the first time in a half-century, India carried out an offensive against China, taking back high ground the Chinese recently grabbed. Chinese forces were surprised when Indian troops mounted their attempt to retake strategic high points. Stunned Chinese soldiers retreated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Then, beginning late last month, for the first time in a half-century, India carried out an offensive against China, taking back high ground the Chinese recently grabbed.',\n",
       " 'Chinese forces were surprised when Indian troops mounted their attempt to retake strategic high points.',\n",
       " 'Stunned Chinese soldiers retreated.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Then',\n",
       " ',',\n",
       " 'beginning',\n",
       " 'late',\n",
       " 'last',\n",
       " 'month',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'in',\n",
       " 'a',\n",
       " 'half-century',\n",
       " ',',\n",
       " 'India',\n",
       " 'carried',\n",
       " 'out',\n",
       " 'an',\n",
       " 'offensive',\n",
       " 'against',\n",
       " 'China',\n",
       " ',',\n",
       " 'taking',\n",
       " 'back',\n",
       " 'high',\n",
       " 'ground',\n",
       " 'the',\n",
       " 'Chinese',\n",
       " 'recently',\n",
       " 'grabbed',\n",
       " '.',\n",
       " 'Chinese',\n",
       " 'forces',\n",
       " 'were',\n",
       " 'surprised',\n",
       " 'when',\n",
       " 'Indian',\n",
       " 'troops',\n",
       " 'mounted',\n",
       " 'their',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'retake',\n",
       " 'strategic',\n",
       " 'high',\n",
       " 'points',\n",
       " '.',\n",
       " 'Stunned',\n",
       " 'Chinese',\n",
       " 'soldiers',\n",
       " 'retreated',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Tokenization(Divide the sentences into words)\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Then,',\n",
       " 'beginning',\n",
       " 'late',\n",
       " 'last',\n",
       " 'month,',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'in',\n",
       " 'a',\n",
       " 'half-century,',\n",
       " 'India',\n",
       " 'carried',\n",
       " 'out',\n",
       " 'an',\n",
       " 'offensive',\n",
       " 'against',\n",
       " 'China,',\n",
       " 'taking',\n",
       " 'back',\n",
       " 'high',\n",
       " 'ground',\n",
       " 'the',\n",
       " 'Chinese',\n",
       " 'recently',\n",
       " 'grabbed.',\n",
       " 'Chinese',\n",
       " 'forces',\n",
       " 'were',\n",
       " 'surprised',\n",
       " 'when',\n",
       " 'Indian',\n",
       " 'troops',\n",
       " 'mounted',\n",
       " 'their',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'retake',\n",
       " 'strategic',\n",
       " 'high',\n",
       " 'points.',\n",
       " 'Stunned',\n",
       " 'Chinese',\n",
       " 'soldiers',\n",
       " 'retreated.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#White Space Tokenization(Tokenize the sentence based on space)\n",
    "WhitespaceTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Then',\n",
       " ',',\n",
       " 'beginning',\n",
       " 'late',\n",
       " 'last',\n",
       " 'month',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'in',\n",
       " 'a',\n",
       " 'half-century',\n",
       " ',',\n",
       " 'India',\n",
       " 'carried',\n",
       " 'out',\n",
       " 'an',\n",
       " 'offensive',\n",
       " 'against',\n",
       " 'China',\n",
       " ',',\n",
       " 'taking',\n",
       " 'back',\n",
       " 'high',\n",
       " 'ground',\n",
       " 'the',\n",
       " 'Chinese',\n",
       " 'recently',\n",
       " 'grabbed.',\n",
       " 'Chinese',\n",
       " 'forces',\n",
       " 'were',\n",
       " 'surprised',\n",
       " 'when',\n",
       " 'Indian',\n",
       " 'troops',\n",
       " 'mounted',\n",
       " 'their',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'retake',\n",
       " 'strategic',\n",
       " 'high',\n",
       " 'points.',\n",
       " 'Stunned',\n",
       " 'Chinese',\n",
       " 'soldiers',\n",
       " 'retreated',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uses regular expressions to tokenize text\n",
    "TreebankWordTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Root Word           \n",
      "Then                then                \n",
      "beginning           begin               \n",
      "late                late                \n",
      "last                last                \n",
      "month               month               \n",
      "first               first               \n",
      "time                time                \n",
      "half-century        half-centuri        \n",
      "India               india               \n",
      "carried             carri               \n",
      "offensive           offens              \n",
      "China               china               \n",
      "taking              take                \n",
      "back                back                \n",
      "high                high                \n",
      "ground              ground              \n",
      "Chinese             chines              \n",
      "recently            recent              \n",
      "grabbed             grab                \n",
      "Chinese             chines              \n",
      "forces              forc                \n",
      "surprised           surpris             \n",
      "Indian              indian              \n",
      "troops              troop               \n",
      "mounted             mount               \n",
      "attempt             attempt             \n",
      "retake              retak               \n",
      "strategic           strateg             \n",
      "high                high                \n",
      "points              point               \n",
      "Stunned             stun                \n",
      "Chinese             chines              \n",
      "soldiers            soldier             \n",
      "retreated           retreat             \n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "sentences = sent_tokenize(sentence)\n",
    "stemmer = PorterStemmer()\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Root Word\"))\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    for word in words:\n",
    "        if (word not in stpwords and word not in panctuations):\n",
    "            print(\"{0:20}{1:20}\".format(word,stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Then                Then                \n",
      "beginning           begin               \n",
      "late                late                \n",
      "last                last                \n",
      "month               month               \n",
      "first               first               \n",
      "time                time                \n",
      "half-century        half-century        \n",
      "India               India               \n",
      "carried             carry               \n",
      "offensive           offensive           \n",
      "China               China               \n",
      "taking              take                \n",
      "back                back                \n",
      "high                high                \n",
      "ground              ground              \n",
      "Chinese             Chinese             \n",
      "recently            recently            \n",
      "grabbed             grab                \n",
      "Chinese             Chinese             \n",
      "forces              force               \n",
      "surprised           surprise            \n",
      "Indian              Indian              \n",
      "troops              troop               \n",
      "mounted             mount               \n",
      "attempt             attempt             \n",
      "retake              retake              \n",
      "strategic           strategic           \n",
      "high                high                \n",
      "points              point               \n",
      "Stunned             Stunned             \n",
      "Chinese             Chinese             \n",
      "soldiers            soldier             \n",
      "retreated           retreat             \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Lemmatization\n",
    "#Function to convert nltk tag to wordnet tag\n",
    "def nltk_to_wordnet(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentences = sent_tokenize(sentence)\n",
    "lematizer = WordNetLemmatizer()\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    nltk_tagged = pos_tag(words)\n",
    "    wordnet_tagged = map(lambda x:(x[0],nltk_to_wordnet(x[1])),nltk_tagged)\n",
    "    for word,tag in wordnet_tagged:\n",
    "        if (word not in stpwords and word not in panctuations):\n",
    "            if tag is not None:\n",
    "                print(\"{0:20}{1:20}\".format(word,lematizer.lemmatize(word,tag)))\n",
    "            else:\n",
    "                print(\"{0:20}{1:20}\".format(word,lematizer.lemmatize(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I----->None\n",
      "am----->v\n",
      "going----->v\n",
      "to----->None\n",
      "school----->n\n"
     ]
    }
   ],
   "source": [
    "nltk_taged = pos_tag(['I','am','going','to','school'])\n",
    "wordnet_tagged = map(lambda x:(x[0],nltk_to_wordnet(x[1])),nltk_taged)\n",
    "for word,tag in wordnet_tagged:\n",
    "    print(f\"{word}----->{tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
